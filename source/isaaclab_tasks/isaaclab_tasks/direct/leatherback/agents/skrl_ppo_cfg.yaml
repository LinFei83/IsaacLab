seed: 42  # 随机种子

# 模型使用 skrl 的模型实例化工具进行实例化
# https://skrl.readthedocs.io/en/latest/api/utils/model_instantiators.html
models:
  separate: False  # 是否使用分离的 actor 和 critic 网络
  policy:  # 策略网络配置 (高斯模型)
    class: GaussianMixin  # 使用高斯混合模型
    clip_actions: False  # 是否裁剪动作范围
    clip_log_std: True  # 是否裁剪对数标准差
    min_log_std: -20.0  # 最小对数标准差
    max_log_std: 2.0  # 最大对数标准差
    initial_log_std: 0.0  # 初始对数标准差
    network:
      - name: net  # 网络名称
        input: STATES  # 输入为状态
        layers: [32, 32] # 神经网络层级结构
        activations: elu  # 激活函数
    output: ACTIONS  # 输出为动作
  value:  # 价值网络配置 (确定性模型)
    class: DeterministicMixin  # 使用确定性混合模型
    clip_actions: False  # 是否裁剪动作范围
    network:
      - name: net  # 网络名称
        input: STATES  # 输入为状态
        layers: [32, 32] # 神经网络层级结构
        activations: elu  # 激活函数
    output: ONE  # 输出为单个值

# 经验回放缓冲区
# https://skrl.readthedocs.io/en/latest/api/memories/random.html
memory:
  class: RandomMemory  # 使用随机内存
  memory_size: -1  # 内存大小，-1 表示自动确定 (与 agent:rollouts 相同)

# PPO 代理配置 (字段名称来自 PPO_DEFAULT_CONFIG)
# https://skrl.readthedocs.io/en/latest/api/agents/ppo.html
agent:
  class: PPO  # 使用 PPO 算法
  rollouts: 32  # 每个环境的 rollout 步数
  learning_epochs: 8  # 学习的 epoch 数量
  mini_batches: 8  # mini-batch 的数量
  discount_factor: 0.99  # 折扣因子
  lambda: 0.95  # GAE (Generalized Advantage Estimation) 的 lambda 参数
  learning_rate: 5.0e-04  # 学习率
  learning_rate_scheduler: KLAdaptiveLR  # 学习率调度器：KL 自适应学习率
  learning_rate_scheduler_kwargs:
    kl_threshold: 0.008  # KL 散度阈值
  state_preprocessor: RunningStandardScaler  # 状态预处理器：运行标准缩放器
  state_preprocessor_kwargs: null  # 状态预处理器参数
  value_preprocessor: RunningStandardScaler  # 价值预处理器：运行标准缩放器
  value_preprocessor_kwargs: null  # 价值预处理器参数
  random_timesteps: 0  # 随机探索的步数
  learning_starts: 0  # 开始学习前的步数
  grad_norm_clip: 1.0  # 梯度裁剪范数
  ratio_clip: 0.2  # PPO 的裁剪比率
  value_clip: 0.2  # PPO 的价值裁剪
  clip_predicted_values: True  # 是否裁剪预测的价值
  entropy_loss_scale: 0.0  # 熵损失系数
  value_loss_scale: 2.0  # 价值损失系数
  kl_threshold: 0.0  # KL 散度阈值 (用于提前停止)
  rewards_shaper_scale: 0.1 # 奖励塑造器缩放比例
  time_limit_bootstrap: False # 是否在时间限制到达时进行自举
  # 日志和检查点
  experiment:
    directory: "leatherback_direct"  # 实验目录
    experiment_name: ""  # 实验名称
    write_interval: auto  # 日志写入间隔
    checkpoint_interval: auto  # 检查点保存间隔

# 顺序训练器
# https://skrl.readthedocs.io/en/latest/api/trainers/sequential.html
trainer:
  class: SequentialTrainer  # 使用顺序训练器
  timesteps: 9600  # 总训练步数
  environment_info: log  # 记录环境信息
