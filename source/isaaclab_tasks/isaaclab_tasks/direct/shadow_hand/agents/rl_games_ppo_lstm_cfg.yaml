# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""
Shadow Hand环境的RL Games PPO LSTM网络算法配置文件。
该配置文件定义了使用RL Games库训练Shadow Hand环境的PPO算法参数，使用LSTM网络结构。
"""

params:
  # 随机种子
  seed: 42

  # 环境包装器裁剪参数
  env:
    # 观测值裁剪阈值
    clip_observations: 5.0
    # 动作裁剪阈值
    clip_actions: 1.0

  # 算法配置
  algo:
    name: a2c_continuous  # 使用连续动作空间的A2C算法

  # 模型配置
  model:
    name: continuous_a2c_logstd  # 连续动作空间的A2C模型，使用对数标准差

  # 网络配置
  network:
    name: actor_critic  # Actor-Critic网络结构
    separate: False  # 是否使用分离的网络

    space:
      continuous:
        mu_activation: None  # 均值激活函数
        sigma_activation: None  # 标准差激活函数
        mu_init:
          name: default  # 均值初始化方法
        sigma_init:
          name: const_initializer  # 标准差初始化方法
          val: 0  # 初始化值
        fixed_sigma: True  # 是否固定标准差
    mlp:
      units: [512]  # MLP层单元数
      activation: relu  # 激活函数
      d2rl: False  # 是否使用D2RL结构

      initializer:
        name: default  # 初始化方法
      regularizer:
        name: None  # 正则化方法
    rnn:
      name: lstm  # RNN类型：LSTM
      units: 1024  # LSTM单元数
      layers: 1  # LSTM层数
      before_mlp: True  # 是否在MLP之前应用RNN
      layer_norm: True  # 是否使用层归一化

  # 检查点加载配置
  load_checkpoint: False  # 是否加载检查点
  load_path: ''  # 检查点路径

  # 训练配置
  config:
    name: shadow_hand_openai_lstm  # 实验名称
    env_name: rlgpu  # 环境名称
    device: 'cuda:0'  # 训练设备
    device_name: 'cuda:0'  # 设备名称
    multi_gpu: False  # 是否使用多GPU
    ppo: True  # 是否使用PPO算法
    mixed_precision: False  # 是否使用混合精度训练
    normalize_input: True  # 是否归一化输入
    normalize_value: True  # 是否归一化价值
    num_actors: -1  # 演员数量（根据环境数量配置）
    reward_shaper:
      scale_value: 0.01  # 奖励缩放因子
    normalize_advantage: True  # 是否归一化优势函数
    gamma: 0.998  # 折扣因子
    tau: 0.95  # GAE参数
    learning_rate: 1e-4  # 学习率
    lr_schedule: adaptive  # 学习率调度
    schedule_type: standard  # 调度类型
    kl_threshold: 0.016  # KL散度阈值
    score_to_win: 100000  # 获胜分数
    max_epochs: 10000  # 最大训练轮数
    save_best_after: 100  # 开始保存最佳模型的轮数
    save_frequency: 200  # 模型保存频率
    print_stats: True  # 是否打印统计信息
    grad_norm: 1.0  # 梯度范数
    entropy_coef: 0.0  # 熵系数
    truncate_grads: True  # 是否截断梯度
    e_clip: 0.2  # PPO裁剪参数
    horizon_length: 16  # 轨迹长度
    minibatch_size: 16384  # 小批量大小
    mini_epochs: 4  # 小轮数
    critic_coef: 4  # 评论家系数
    clip_value: True  # 是否裁剪价值
    seq_length: 4  # 序列长度
    bounds_loss_coef: 0.0001  # 边界损失系数

    # 中心价值配置（用于非对称观测）
    central_value_config:
      minibatch_size: 32768  # 小批量大小
      mini_epochs: 4  # 小轮数
      learning_rate: 1e-4  # 学习率
      kl_threshold: 0.016  # KL散度阈值
      clip_value: True  # 是否裁剪价值
      normalize_input: True  # 是否归一化输入
      truncate_grads: True  # 是否截断梯度

      network:
        name: actor_critic  # 网络名称
        central_value: True  # 是否为中心价值网络
        mlp:
          units: [512]  # MLP层单元数
          activation: relu  # 激活函数
          d2rl: False  # 是否使用D2RL结构
          initializer:
            name: default  # 初始化方法
          regularizer:
            name: None  # 正则化方法
        rnn:
          name: lstm  # RNN类型：LSTM
          units: 1024  # LSTM单元数
          layers: 1  # LSTM层数
          before_mlp: True  # 是否在MLP之前应用RNN
          layer_norm: True  # 是否使用层归一化
          zero_rnn_on_done: False  # 是否在完成时清零RNN状态

    # 玩家配置（推理时使用）
    player:
      deterministic: True  # 是否使用确定性策略
      games_num: 100000  # 游戏数量
      print_stats: True  # 是否打印统计信息
